{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommendation system requires training with each new items/user on our platform, so it's reasonable to make a continues training pipleline \"Kubeflow\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up GCP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the project id here\n",
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "REGION = 'us-central1' # Define your zone here\n",
    "BUCKET = 'gs://' + PROJECT_ID # Our bucket, feel free to change it as fits to your project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excute next cell if you didn't make your bucker yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsutil mb -p $PROJECT_ID gs://$PROJECT_ID\n",
    "gsutil acl ch -u AllUsers:R gs://$PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PRIOR TO STARTING THE LAB: Make sure you create a new instance with AI Platform Pipelines. Once the GKE cluster is spun up, copy the endpoint because you will need it in this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "END_POINT = \"\" #Your cluster endpoint #GKE host url\n",
    "PIPELINE_NAME = \"Rec_Anime_NNFC_TF\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must make sure all apis are enabled, run the next cell to enable it. \"We can run it in cloud shell as well.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bash: line 1: gcloud: command not found\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'gcloud services enable \\\\\\n  serviceusage.googleapis.com \\\\\\n  compute.googleapis.com \\\\\\n  container.googleapis.com \\\\\\n  iam.googleapis.com \\\\\\n  servicemanagement.googleapis.com \\\\\\n  cloudresourcemanager.googleapis.com \\\\\\n  ml.googleapis.com \\\\\\n  iap.googleapis.com \\\\\\n  sqladmin.googleapis.com \\\\\\n  meshconfig.googleapis.com \\\\\\n  krmapihosting.googleapis.com \\\\\\n  servicecontrol.googleapis.com \\\\\\n  endpoints.googleapis.com\\n'' returned non-zero exit status 127.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbash\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgcloud services enable \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m  serviceusage.googleapis.com \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m  compute.googleapis.com \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m  container.googleapis.com \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m  iam.googleapis.com \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m  servicemanagement.googleapis.com \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m  cloudresourcemanager.googleapis.com \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m  ml.googleapis.com \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m  iap.googleapis.com \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m  sqladmin.googleapis.com \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m  meshconfig.googleapis.com \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m  krmapihosting.googleapis.com \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m  servicecontrol.googleapis.com \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m  endpoints.googleapis.com\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/akm/My Work/Programming/virt-env/recommender-system/lib/python3.9/site-packages/IPython/core/interactiveshell.py:2417\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2415\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2416\u001b[0m     args \u001b[39m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2417\u001b[0m     result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   2418\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/media/akm/My Work/Programming/virt-env/recommender-system/lib/python3.9/site-packages/IPython/core/magics/script.py:153\u001b[0m, in \u001b[0;36mScriptMagics._make_script_magic.<locals>.named_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     line \u001b[39m=\u001b[39m script\n\u001b[0;32m--> 153\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshebang(line, cell)\n",
      "File \u001b[0;32m/media/akm/My Work/Programming/virt-env/recommender-system/lib/python3.9/site-packages/IPython/core/magics/script.py:305\u001b[0m, in \u001b[0;36mScriptMagics.shebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[39mif\u001b[39;00m args\u001b[39m.\u001b[39mraise_error \u001b[39mand\u001b[39;00m p\u001b[39m.\u001b[39mreturncode \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    301\u001b[0m     \u001b[39m# If we get here and p.returncode is still None, we must have\u001b[39;00m\n\u001b[1;32m    302\u001b[0m     \u001b[39m# killed it but not yet seen its return code. We don't wait for it,\u001b[39;00m\n\u001b[1;32m    303\u001b[0m     \u001b[39m# in case it's stuck in uninterruptible sleep. -9 = SIGKILL\u001b[39;00m\n\u001b[1;32m    304\u001b[0m     rc \u001b[39m=\u001b[39m p\u001b[39m.\u001b[39mreturncode \u001b[39mor\u001b[39;00m \u001b[39m-\u001b[39m\u001b[39m9\u001b[39m\n\u001b[0;32m--> 305\u001b[0m     \u001b[39mraise\u001b[39;00m CalledProcessError(rc, cell)\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'gcloud services enable \\\\\\n  serviceusage.googleapis.com \\\\\\n  compute.googleapis.com \\\\\\n  container.googleapis.com \\\\\\n  iam.googleapis.com \\\\\\n  servicemanagement.googleapis.com \\\\\\n  cloudresourcemanager.googleapis.com \\\\\\n  ml.googleapis.com \\\\\\n  iap.googleapis.com \\\\\\n  sqladmin.googleapis.com \\\\\\n  meshconfig.googleapis.com \\\\\\n  krmapihosting.googleapis.com \\\\\\n  servicecontrol.googleapis.com \\\\\\n  endpoints.googleapis.com\\n'' returned non-zero exit status 127."
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud services enable \\\n",
    "  serviceusage.googleapis.com \\\n",
    "  compute.googleapis.com \\\n",
    "  container.googleapis.com \\\n",
    "  iam.googleapis.com \\\n",
    "  servicemanagement.googleapis.com \\\n",
    "  cloudresourcemanager.googleapis.com \\\n",
    "  ml.googleapis.com \\\n",
    "  iap.googleapis.com \\\n",
    "  sqladmin.googleapis.com \\\n",
    "  meshconfig.googleapis.com \\\n",
    "  krmapihosting.googleapis.com \\\n",
    "  servicecontrol.googleapis.com \\\n",
    "  endpoints.googleapis.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NUMBER=$(gcloud projects describe $PROJECT_ID --format=\"value(projectNumber)\")\n",
    "CLOUD_BUILD_SERVICE_ACCOUNT=\"${PROJECT_NUMBER}@cloudbuild.gserviceaccount.com\"\n",
    "gcloud projects add-iam-policy-binding $PROJECT_ID \\\n",
    "  --member serviceAccount:$CLOUD_BUILD_SERVICE_ACCOUNT \\\n",
    "  --role roles/editor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Anthos Service Mesh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Anthos Service Mesh, you get an Anthos tested and supported distribution of Istio, letting you create and deploy a service mesh on GKE on Google Cloud and other platforms with full Google support. \"Needed for Google Kubernetes Engine\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "curl --request POST \\\n",
    "  --header \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n",
    "  --data '' \\\n",
    "  https://meshconfig.googleapis.com/v1alpha1/projects/${PROJECT_ID}:initialize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define our training script here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "train_script_dir = \"NNFC\" # Our training scripts will be saved in this directory\n",
    "if not os.path.exists(train_script_dir): \n",
    "    os.makedirs(train_script_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./NNFC/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./NNFC/train.py\n",
    "\n",
    "import fire\n",
    "import pandas as pd\n",
    "import os\n",
    "import config\n",
    "from Utils.data.preprocess import preprocessor_anime_data,preprocess_colabritive\n",
    "from train.colabritive_system import NNCollaborativeFiltering\n",
    "import pickle\n",
    "import subprocess\n",
    "\n",
    "def save_model(save_dir,model,save_tf: bool=True):\n",
    "    \"\"\"args:\n",
    "            save_dir: The model registry path to save the model.\n",
    "            model: The trained model we want to save.\n",
    "            save_tf: if True save it in tensorflow formate, otherwise saves it as picke.\n",
    "    \"\"\"\n",
    "\n",
    "    model_name = os.path.join(\"save_dir\",\"anime_recommender\")\n",
    "    if save_tf:\n",
    "        model.save(f\"{model_name}.h5\")\n",
    "    else:\n",
    "        with open(model_name, 'wb') as model_file:\n",
    "        pickle.dump(model, f\"{model_name}.pkl\")\n",
    "\n",
    "def train(training_dataset_path,output_dir,epochs):\n",
    "    path_anime = os.path.join(\"training_dataset_path\", \"anime.csv\")\n",
    "    path_anime_list = os.path.join(\"training_dataset_path\", \"animelist.csv\")\n",
    "\n",
    "    # I want to test on user_id 0\n",
    "    r_anime = pd.read_csv(path_anime,low_memory=True) # THat's the maximum for that\n",
    "\n",
    "    anime_data = preprocessor_anime_data(r_anime).get_transformed_data()\n",
    "    # Data behave differently when loading next row\n",
    "    prepro_nnfc_class = preprocess_colabritive(path_anime_list,load_rows=88) # That's why SQL is important\n",
    "\n",
    "    x_user,x_item,y = prepro_nnfc_class.get_x_y_data_NNCF(my_class.get_users_for_item(),anime_data)\n",
    "    n_users,n_items = prepro_nnfc_class.get_num_user_items()\n",
    "\n",
    "    model = NNCollaborativeFiltering(n_users=n_users, n_items=n_items)\n",
    "    history,model = model.train_model(x_user,x_item,y,epochs=epochs,embedding_dims=10, d_layers=[10]) # The rest of training params could be added in func param as well\n",
    "    \n",
    "    save_model(output_dir,model,True)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    fire.Fire(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./NNFC/Dockerfile\n",
    "\n",
    "FROM gcr.io/deeplearning-platform-release/base-cpu\n",
    "RUN pip install -U fire tensorflow==2.10 pandas scikit-learn\n",
    "WORKDIR /app\n",
    "COPY NFFC/train.py .\n",
    "COPY Utils/* .\n",
    "COPY train/colabritive_system.py .\n",
    "\n",
    "ENTRYPOINT [\"python\", \"train.py\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "RECOM_NNFC_IMAGE_NAME='recomm_nnfc_image'\n",
    "RECOM_NNFC_IMAGE_TAG='latest'\n",
    "RECOM_NNFC_IMAGE_URI=f'gcr.io/{PROJECT_ID}/{RECOM_NNFC_IMAGE_NAME}:{RECOM_NNFC_IMAGE_TAG}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud builds submit --tag $SCIKIT_IMAGE_URI $SCIKIT_IMAGE_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./pipeline/recom_anime_tf_pipeline.py\n",
    "\n",
    "import os\n",
    "import kfp\n",
    "from kfp.dsl.types import GCPProjectID\n",
    "from kfp.dsl.types import GCPRegion\n",
    "from kfp.dsl.types import GCSPath\n",
    "from kfp.dsl.types import String\n",
    "from kfp.gcp import use_gcp_secret\n",
    "import kfp.components as comp, create_component_from_func\n",
    "import kfp.dsl as dsl\n",
    "import kfp.gcp as gcp\n",
    "\n",
    "TF_TRAINER_IMAGE = os.getenv('RECOM_NNFC_IMAGE_NAME')\n",
    "BUCKET = os.getenv('BUCKET')\n",
    "\n",
    "# Paths to export the training/validation data from bigquery\n",
    "TRAINING_OUTPUT_PATH = BUCKET + '/census/data/training.csv'\n",
    "VALIDATION_OUTPUT_PATH = BUCKET + '/census/data/validation.csv'\n",
    "\n",
    "COMPONENT_URL_SEARCH_PREFIX = 'https://raw.githubusercontent.com/kubeflow/pipelines/0.2.5/components/gcp/'\n",
    "\n",
    "# Create component factories\n",
    "component_store = kfp.components.ComponentStore(\n",
    "    local_search_paths=None, url_search_prefixes=[COMPONENT_URL_SEARCH_PREFIX])\n",
    "\n",
    "# Load BigQuery and AI Platform Training op\n",
    "mlengine_train_op = component_store.load_component('ml_engine/train')\n",
    "\n",
    "\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name='Recom_Anime_NNFC_Pipeline',\n",
    "    description='Pipeline continuesly train recommender system for animes, NNFC model'\n",
    ")\n",
    "def pipeline(\n",
    "    project_id,\n",
    "    region='us-central1'\n",
    "):\n",
    "\n",
    "\n",
    "    # These are the output directories where our models will be saved\n",
    "    tf_output_dir = BUCKET + '/Rec-Anime/models/tf'\n",
    "    \n",
    "    # Training arguments to be passed to the TF Trainer\n",
    "    tf_args = [\n",
    "        '--training_dataset_path', create_training_split.outputs['output_gcs_path'],\n",
    "        '--output_dir', tf_output_dir,\n",
    "        '--epochs', '130',\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    # AI Platform Training Jobs with trainer images \n",
    "    train_tf = mlengine_train_op(\n",
    "        project_id=project_id,\n",
    "        region=region,\n",
    "        master_image_uri=TF_TRAINER_IMAGE,\n",
    "        args=tf_args).set_display_name('Tensorflow Anime Recommender NNFC Model - AI Platform Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We Must set our training image name to use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAG = 'latest'\n",
    "TF_TRAINER_IMAGE = f'gcr.io/{PROJECT_ID}/tensorflow_trainer_image:{TAG}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TF_TRAINER_IMAGE=gcr.io//bin/bash: gcloud: command not found/tensorflow_trainer_image:latest\n"
     ]
    }
   ],
   "source": [
    "%env TF_TRAINER_IMAGE={TF_TRAINER_IMAGE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!dsl-compile --py ./pipeline/recom_anime_tf_pipeline.py --output recom_anime_tf_pipeline.yaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head recom_anime_tf_pipeline.yaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sed -i 's/\\\"command\\\": \\[\\]/\\\"command\\\": \\[python, -u, -m, kfp_component.launcher\\]/g' recom_anime_tf_pipeline.yaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat recom_anime_tf_pipeline.yaml | grep \"component.launcher\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kfp --endpoint $ENDPOINT pipeline upload \\\n",
    "-p $PIPELINE_NAME \\\n",
    "./recom_anime_tf_pipeline.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Set necessary variables: \n",
    "MODEL_NAME=\"Rec-Anime-NNFC-TF\"\n",
    "MODEL_VERSION=\"1.0\"\n",
    "MODEL_LOCATION=$(gsutil ls gs://${BUCKET}/babyweight/export/exporter/ | tail -1)\n",
    "\n",
    "# Set the region to global by executing the following command: \n",
    "gcloud config set ai_platform/region global\n",
    "\n",
    "echo \"Deploying the model '$MODEL_NAME', version '$MODEL_VERSION' from $MODEL_LOCATION\"\n",
    "echo \"... this will take a few minutes\"\n",
    "\n",
    "# Deploy trained model: \n",
    "gcloud ai-platform models create ${MODEL_NAME} --regions $REGION\n",
    "# Create a new AI Platform version.\n",
    "# TODO\n",
    "gcloud ai-platform versions create ${MODEL_VERSION} \\\n",
    "  --model ${MODEL_NAME} \\\n",
    "  --origin ${MODEL_LOCATION} \\\n",
    "  --runtime-version $TFVERSION"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('recommender-system')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "286d4d870da66a04ade5647f6fd5517d1e47378233caa3ff00ed5744fc70a6ec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
